{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "## Data Engineering Capstone Project\n",
    "\n",
    "### Project Summary\n",
    "In this project we will build an ETL pipeline to extract data from different sources clean and transform it to a usefull and easy to understand star schema and save it for future usage.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql.types import MapType\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Date, TimestampType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "### Scope \n",
    "This project will integrate I94 immigration data, Airport Codes data and US demographic data to setup a data warehouse with fact and dimension tables.\n",
    "\n",
    "### Describe and Gather Data \n",
    "**Data Sets**\n",
    "- [I94 Immigration Data](https://www.trade.gov/national-travel-and-tourism-office)\n",
    "    Data contains international visitor arrival statistics by world regions and select countries (including top 20), type of visa, mode of transportation, age groups, states visited (first intended address only), and the top ports of entry (for select countries).\n",
    "- [Airport codes Data](https://datahub.io/core/airport-codes#data)\n",
    "   This data contains the list of all airport codes, names, location, elevation etc.\n",
    "- [U.S. City Demographic Data](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)\n",
    "    This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000.\n",
    "\n",
    "**Tools**\n",
    "- AWS S3: data storage\n",
    "- Python for data processing\n",
    "    - Pandas - exploratory data analysis on small data set\n",
    "    - PySpark - data processing on large data set\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()\n",
    "\n",
    "df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "# df_spark.write.parquet(\"sas_data\")\n",
    "df_spark=spark.read.parquet(\"sas_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 3096313|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.createOrReplaceTempView(\"df_spark\")\n",
    "spark.sql(\"\"\"SELECT COUNT(*)\n",
    "                FROM df_spark\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explore and Assess the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I94-immigration data:\n",
    "\n",
    "#### Explore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  \\\n",
       "0     2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0   \n",
       "1     2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0   \n",
       "2      589494  1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0   \n",
       "3     2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0   \n",
       "4     3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0   \n",
       "\n",
       "   i94mode i94addr    ...     entdepu  matflag  biryear   dtaddto  gender  \\\n",
       "0      1.0      HI    ...         NaN        M   1955.0  07202016       F   \n",
       "1      1.0      TX    ...         NaN        M   1990.0  10222016       M   \n",
       "2      1.0      FL    ...         NaN        M   1940.0  07052016       M   \n",
       "3      1.0      CA    ...         NaN        M   1991.0  10272016       M   \n",
       "4      3.0      NY    ...         NaN        M   1997.0  07042016       F   \n",
       "\n",
       "  insnum airline        admnum  fltno  visatype  \n",
       "0    NaN      JL  5.658267e+10  00782        WT  \n",
       "1    NaN     *GA  9.436200e+10  XBLNG        B2  \n",
       "2    NaN      LH  5.578047e+10  00464        WT  \n",
       "3    NaN      QR  9.478970e+10  00739        B2  \n",
       "4    NaN     NaN  4.232257e+10   LAND        WT  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"immigration_data_sample.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Missing Values:** This data has missing values in many columns. \n",
    "- **Useless Columns:** This data has many useless columns which are either completely empty or have majority of missing values.\n",
    "- **Column Names:** Column names have been abbreviated and confusing It would be better to give them more precise names.\n",
    "- **Column Types:** Many columns have unnecassarily big float types I shall give them more generic integer types.\n",
    "- **Coded Columns** Many columns have coded values in them e.g. i94cit,i94res. Lets give them their original values where more optimizable.\n",
    "- **Dates:** Dates are in weird formats It would be better to give them more generic timestamp format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cicid</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>citizen_country</th>\n",
       "      <th>residence_country</th>\n",
       "      <th>port</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>departure_date</th>\n",
       "      <th>state_code</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>visa</th>\n",
       "      <th>gender</th>\n",
       "      <th>adm_num</th>\n",
       "      <th>flight_no</th>\n",
       "      <th>airline</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5748517</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>245</td>\n",
       "      <td>438</td>\n",
       "      <td>LOS</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>2016-05-08</td>\n",
       "      <td>CA</td>\n",
       "      <td>40</td>\n",
       "      <td>Business</td>\n",
       "      <td>F</td>\n",
       "      <td>94953870030</td>\n",
       "      <td>00011</td>\n",
       "      <td>QF</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5748518</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>245</td>\n",
       "      <td>438</td>\n",
       "      <td>LOS</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>2016-05-17</td>\n",
       "      <td>NV</td>\n",
       "      <td>32</td>\n",
       "      <td>Business</td>\n",
       "      <td>F</td>\n",
       "      <td>94955622830</td>\n",
       "      <td>00007</td>\n",
       "      <td>VA</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5748519</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>245</td>\n",
       "      <td>438</td>\n",
       "      <td>LOS</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>2016-05-08</td>\n",
       "      <td>WA</td>\n",
       "      <td>29</td>\n",
       "      <td>Business</td>\n",
       "      <td>M</td>\n",
       "      <td>94956406530</td>\n",
       "      <td>00040</td>\n",
       "      <td>DL</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5748520</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>245</td>\n",
       "      <td>438</td>\n",
       "      <td>LOS</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>2016-05-14</td>\n",
       "      <td>WA</td>\n",
       "      <td>29</td>\n",
       "      <td>Business</td>\n",
       "      <td>F</td>\n",
       "      <td>94956451430</td>\n",
       "      <td>00040</td>\n",
       "      <td>DL</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5748521</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>245</td>\n",
       "      <td>438</td>\n",
       "      <td>LOS</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>2016-05-14</td>\n",
       "      <td>WA</td>\n",
       "      <td>28</td>\n",
       "      <td>Business</td>\n",
       "      <td>M</td>\n",
       "      <td>94956388130</td>\n",
       "      <td>00040</td>\n",
       "      <td>DL</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5748522</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>245</td>\n",
       "      <td>464</td>\n",
       "      <td>HHW</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>2016-05-05</td>\n",
       "      <td>HI</td>\n",
       "      <td>57</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>M</td>\n",
       "      <td>94981802830</td>\n",
       "      <td>00010</td>\n",
       "      <td>NZ</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>5748523</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>245</td>\n",
       "      <td>464</td>\n",
       "      <td>HHW</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>HI</td>\n",
       "      <td>66</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>F</td>\n",
       "      <td>94979689930</td>\n",
       "      <td>00010</td>\n",
       "      <td>NZ</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>5748524</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>245</td>\n",
       "      <td>464</td>\n",
       "      <td>HHW</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>HI</td>\n",
       "      <td>41</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>F</td>\n",
       "      <td>94979746730</td>\n",
       "      <td>00010</td>\n",
       "      <td>NZ</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>5748525</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>245</td>\n",
       "      <td>464</td>\n",
       "      <td>HOU</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>2016-05-07</td>\n",
       "      <td>FL</td>\n",
       "      <td>27</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>M</td>\n",
       "      <td>94973246630</td>\n",
       "      <td>00028</td>\n",
       "      <td>NZ</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>5748526</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>245</td>\n",
       "      <td>464</td>\n",
       "      <td>LOS</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>2016-05-07</td>\n",
       "      <td>CA</td>\n",
       "      <td>26</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>F</td>\n",
       "      <td>95013547930</td>\n",
       "      <td>00002</td>\n",
       "      <td>NZ</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    cicid  year  month  citizen_country  residence_country port  \\\n",
       "0   0  5748517  2016      4              245                438  LOS   \n",
       "1   1  5748518  2016      4              245                438  LOS   \n",
       "2   2  5748519  2016      4              245                438  LOS   \n",
       "3   3  5748520  2016      4              245                438  LOS   \n",
       "4   4  5748521  2016      4              245                438  LOS   \n",
       "5   5  5748522  2016      4              245                464  HHW   \n",
       "6   6  5748523  2016      4              245                464  HHW   \n",
       "7   7  5748524  2016      4              245                464  HHW   \n",
       "8   8  5748525  2016      4              245                464  HOU   \n",
       "9   9  5748526  2016      4              245                464  LOS   \n",
       "\n",
       "  arrival_date departure_date state_code  birth_year      visa gender  \\\n",
       "0   2016-04-30     2016-05-08         CA          40  Business      F   \n",
       "1   2016-04-30     2016-05-17         NV          32  Business      F   \n",
       "2   2016-04-30     2016-05-08         WA          29  Business      M   \n",
       "3   2016-04-30     2016-05-14         WA          29  Business      F   \n",
       "4   2016-04-30     2016-05-14         WA          28  Business      M   \n",
       "5   2016-04-30     2016-05-05         HI          57  Pleasure      M   \n",
       "6   2016-04-30     2016-05-12         HI          66  Pleasure      F   \n",
       "7   2016-04-30     2016-05-12         HI          41  Pleasure      F   \n",
       "8   2016-04-30     2016-05-07         FL          27  Pleasure      M   \n",
       "9   2016-04-30     2016-05-07         CA          26  Pleasure      F   \n",
       "\n",
       "       adm_num flight_no airline visatype  \n",
       "0  94953870030     00011      QF       B1  \n",
       "1  94955622830     00007      VA       B1  \n",
       "2  94956406530     00040      DL       B1  \n",
       "3  94956451430     00040      DL       B1  \n",
       "4  94956388130     00040      DL       B1  \n",
       "5  94981802830     00010      NZ       B2  \n",
       "6  94979689930     00010      NZ       B2  \n",
       "7  94979746730     00010      NZ       B2  \n",
       "8  94973246630     00028      NZ       B2  \n",
       "9  95013547930     00002      NZ       B2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing cleaning tasks here\n",
    "spark.udf.register(\"SAS_to_datetime\",lambda date: pd.to_timedelta(date, unit='D') + pd.Timestamp('1960-1-1'),TimestampType())\n",
    "def visa_int_to_var(visa):\n",
    "    if visa == 1:\n",
    "        return 'Business'\n",
    "    elif visa == 2:\n",
    "        return 'Pleasure'\n",
    "    elif visa == 3:\n",
    "        return 'Student'\n",
    "spark.udf.register(\"visa_transform\",lambda visa:visa_int_to_var(visa))\n",
    "df_spark.createOrReplaceTempView(\"df_spark\")\n",
    "cleaned_df_immi = spark.sql(\"\"\"SELECT monotonically_increasing_id() as id,\n",
    "                            cast(cicid as BIGINT),\n",
    "                            cast(i94yr as INT) as year,\n",
    "                            cast(i94mon as INT) as month,\n",
    "                            cast(i94cit as INT) as citizen_country,\n",
    "                            cast(i94res as INT) as residence_country,\n",
    "                            cast(i94port as CHAR(3)) as port,\n",
    "                            SAS_to_datetime(arrdate) as arrival_date,\n",
    "                            SAS_to_datetime(depdate) as departure_date,\n",
    "                            cast(i94addr as CHAR(2)) as state_code,\n",
    "                            cast(i94bir as INT) as birth_year,\n",
    "                            visa_transform(i94visa) as visa,\n",
    "                            cast(gender as CHAR(1)) as gender,\n",
    "                            cast(admnum as BIGINT) as adm_num,\n",
    "                            fltno as flight_no,\n",
    "                            airline,\n",
    "                            visatype\n",
    "                            FROM df_spark\n",
    "                            WHERE i94mode = 1 AND depdate IS NOT NULL AND i94addr IS NOT NULL\n",
    "                            AND fltno IS NOT NULL AND airline IS NOT NULL AND visatype IS NOT NULL\n",
    "                            AND gender IS NOT NULL AND i94cit IS NOT NULL AND i94res IS NOT NULL\n",
    "                            \"\"\")\n",
    "\n",
    "cleaned_df_immi.createOrReplaceTempView(\"df_immi\")\n",
    "cleaned_df_immi.limit(10).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracing Country codes and Country names from I94_SAS_Labels_Description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+\n",
      "|country_code|   country_name|\n",
      "+------------+---------------+\n",
      "|         236|    AFGHANISTAN|\n",
      "|         101|        ALBANIA|\n",
      "|         316|        ALGERIA|\n",
      "|         102|        ANDORRA|\n",
      "|         324|         ANGOLA|\n",
      "|         529|       ANGUILLA|\n",
      "|         518|ANTIGUA-BARBUDA|\n",
      "|         687|     ARGENTINA |\n",
      "|         151|        ARMENIA|\n",
      "|         532|          ARUBA|\n",
      "|         438|      AUSTRALIA|\n",
      "|         103|        AUSTRIA|\n",
      "|         152|     AZERBAIJAN|\n",
      "|         512|        BAHAMAS|\n",
      "|         298|        BAHRAIN|\n",
      "|         274|     BANGLADESH|\n",
      "|         513|       BARBADOS|\n",
      "|         104|        BELGIUM|\n",
      "|         581|         BELIZE|\n",
      "|         386|          BENIN|\n",
      "+------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def parseco(line):\n",
    "    import re\n",
    "    pattern = \"^\\s*(\\d+)\\s*=\\s*'(.*)'\"\n",
    "    match = re.search(pattern,line)\n",
    "\n",
    "    return {\n",
    "        'code' :int(match.group(1)),\n",
    "        'country':match.group(2)\n",
    "        }\n",
    "spark.udf.register('parseco',parseco,MapType(Str(),Str()))\n",
    "df=spark.read.text(\"I94_SAS_Labels_Descriptions.SAS\")\n",
    "df_countries=spark.createDataFrame(df.collect()[10:298])\n",
    "df_countries.createOrReplaceTempView(\"df_co\")\n",
    "df_countries=spark.sql(\"\"\"SELECT parseco(value) as parsed\n",
    "                            FROM df_co\"\"\")\n",
    "df_countries.createOrReplaceTempView(\"df_co\")\n",
    "df_countries = spark.sql(\"\"\"SELECT parsed['code'] as country_code,\n",
    "                                    parsed['country'] as country_name\n",
    "                                FROM df_co\"\"\")\n",
    "df_countries.createOrReplaceTempView(\"df_countries\")\n",
    "df_countries.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting city codes and city names from I94_SAS_Labels_Description:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like this one needs a bit of cleaning too because city_names and states are merged together in strings and we need to parse them apart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_code</th>\n",
       "      <th>city_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANC</td>\n",
       "      <td>ANCHORAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BAR</td>\n",
       "      <td>BAKER AAF - BAKER ISLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DAC</td>\n",
       "      <td>DALTONS CACHE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PIZ</td>\n",
       "      <td>DEW STATION PT LAY DEW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DTH</td>\n",
       "      <td>DUTCH HARBOR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  city_code                 city_name\n",
       "0       ANC                 ANCHORAGE\n",
       "1       BAR  BAKER AAF - BAKER ISLAND\n",
       "2       DAC             DALTONS CACHE\n",
       "3       PIZ    DEW STATION PT LAY DEW\n",
       "4       DTH              DUTCH HARBOR"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parsecity(line):\n",
    "    import re\n",
    "    pattern = \"^\\s*'(\\w{3})'\\s*=\\s*'([\\w\\s\\./-]*),{0,1}\"\n",
    "    match = re.search(pattern,line)\n",
    "\n",
    "    return {\n",
    "        'city_code' :match.group(1),\n",
    "        'city_name':match.group(2)\n",
    "        }\n",
    "spark.udf.register('parsecity',parsecity,MapType(Str(),Str()))\n",
    "df_city=spark.createDataFrame(df.collect()[303:893])\n",
    "df_city.createOrReplaceTempView(\"df_city\")\n",
    "df_city=spark.sql(\"\"\"SELECT parsecity(value) as parsed\n",
    "                FROM df_city\"\"\")\n",
    "df_city.createOrReplaceTempView(\"df_city\")\n",
    "df_city=spark.sql(\"\"\"SELECT parsed['city_code'] as city_code,parsed['city_name'] as city_name\n",
    "                        FROM df_city\"\"\")\n",
    "df_city.createOrReplaceTempView(\"df_city\")\n",
    "df_city.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting State Codes and State names from I94_SAS_Labels_Description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|state_code|       state_name|\n",
      "+----------+-----------------+\n",
      "|        AK|           ALASKA|\n",
      "|        AZ|          ARIZONA|\n",
      "|        AR|         ARKANSAS|\n",
      "|        CA|       CALIFORNIA|\n",
      "|        CO|         COLORADO|\n",
      "|        CT|      CONNECTICUT|\n",
      "|        DE|         DELAWARE|\n",
      "|        DC|DIST. OF COLUMBIA|\n",
      "|        FL|          FLORIDA|\n",
      "|        GA|          GEORGIA|\n",
      "|        GU|             GUAM|\n",
      "|        HI|           HAWAII|\n",
      "|        ID|            IDAHO|\n",
      "|        IL|         ILLINOIS|\n",
      "|        IN|          INDIANA|\n",
      "|        IA|             IOWA|\n",
      "|        KS|           KANSAS|\n",
      "|        KY|         KENTUCKY|\n",
      "|        LA|        LOUISIANA|\n",
      "|        ME|            MAINE|\n",
      "+----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def parsestate(line):\n",
    "    import re\n",
    "    pattern = \"^\\s*'(\\w{2})'\\s*=\\s*'(.*)'\"\n",
    "    match = re.search(pattern,line)\n",
    "\n",
    "    return {\n",
    "        'state_code' :match.group(1),\n",
    "        'state_name':match.group(2)\n",
    "        }\n",
    "spark.udf.register('parsestate',parsestate,MapType(Str(),Str()))\n",
    "df_state=spark.createDataFrame(df.collect()[982:1036])\n",
    "df_state.createOrReplaceTempView(\"df_state\")\n",
    "df_state=spark.sql(\"\"\"SELECT parsestate(value) as parsed\n",
    "                            FROM df_state\"\"\")\n",
    "df_state.createOrReplaceTempView(\"df_state\")\n",
    "df_state = spark.sql(\"\"\"SELECT parsed['state_code'] as state_code,\n",
    "                            parsed['state_name'] as state_name\n",
    "                            FROM df_state\"\"\")\n",
    "df_state.createOrReplaceTempView(\"df_state\")\n",
    "df_state.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US-Cities_Demographics Data:\n",
    "#### Exploring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"us-cities-demographics.csv\",delimiter=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning us-cities-demographics data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Column Names:** Column names have been abbreviated and confusing It would be better to give them more precise names.\n",
    "- **Duplicate data:** There is a lot of duplicate data here i.e. last count provides counts for each unique race in the a provided city but one simple observation was that because of this unique extra feature rest of the columns are just duplicates of each other so I have decided to only extract the majority race for each city from this data. and drop the rest rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>median_age</th>\n",
       "      <th>male_population</th>\n",
       "      <th>female_population</th>\n",
       "      <th>total_population</th>\n",
       "      <th>veteran_population</th>\n",
       "      <th>foreign_population</th>\n",
       "      <th>avg_household_size</th>\n",
       "      <th>state_code</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QUINCY</td>\n",
       "      <td>MASSACHUSETTS</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129</td>\n",
       "      <td>49500</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147</td>\n",
       "      <td>32935</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FORT MYERS</td>\n",
       "      <td>FLORIDA</td>\n",
       "      <td>37.3</td>\n",
       "      <td>36850</td>\n",
       "      <td>37165</td>\n",
       "      <td>74015</td>\n",
       "      <td>4312</td>\n",
       "      <td>15365</td>\n",
       "      <td>2.45</td>\n",
       "      <td>FL</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PITTSBURGH</td>\n",
       "      <td>PENNSYLVANIA</td>\n",
       "      <td>32.9</td>\n",
       "      <td>149690</td>\n",
       "      <td>154695</td>\n",
       "      <td>304385</td>\n",
       "      <td>17728</td>\n",
       "      <td>28187</td>\n",
       "      <td>2.13</td>\n",
       "      <td>PA</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAMPTON</td>\n",
       "      <td>VIRGINIA</td>\n",
       "      <td>35.5</td>\n",
       "      <td>66214</td>\n",
       "      <td>70240</td>\n",
       "      <td>136454</td>\n",
       "      <td>19638</td>\n",
       "      <td>6204</td>\n",
       "      <td>2.48</td>\n",
       "      <td>VA</td>\n",
       "      <td>Black or African-American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FREDERICK</td>\n",
       "      <td>MARYLAND</td>\n",
       "      <td>36.1</td>\n",
       "      <td>33146</td>\n",
       "      <td>36336</td>\n",
       "      <td>69482</td>\n",
       "      <td>3870</td>\n",
       "      <td>14211</td>\n",
       "      <td>2.48</td>\n",
       "      <td>MD</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         city          state  median_age  male_population  female_population  \\\n",
       "0      QUINCY  MASSACHUSETTS        41.0            44129              49500   \n",
       "1  FORT MYERS        FLORIDA        37.3            36850              37165   \n",
       "2  PITTSBURGH   PENNSYLVANIA        32.9           149690             154695   \n",
       "3     HAMPTON       VIRGINIA        35.5            66214              70240   \n",
       "4   FREDERICK       MARYLAND        36.1            33146              36336   \n",
       "\n",
       "   total_population  veteran_population  foreign_population  \\\n",
       "0             93629                4147               32935   \n",
       "1             74015                4312               15365   \n",
       "2            304385               17728               28187   \n",
       "3            136454               19638                6204   \n",
       "4             69482                3870               14211   \n",
       "\n",
       "   avg_household_size state_code                       race  \n",
       "0                2.39         MA                      White  \n",
       "1                2.45         FL                      White  \n",
       "2                2.13         PA                      White  \n",
       "3                2.48         VA  Black or African-American  \n",
       "4                2.48         MD                      White  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.csv(\"us-cities-demographics.csv\",sep=\";\", inferSchema=True, header=True)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "df_us_cities=spark.sql(\"\"\"\n",
    "                    SELECT upper(a.city) as city,\n",
    "                    upper(a.state) as state,\n",
    "                    a.`Median Age` as median_age,\n",
    "                    a.`Male Population` as male_population,\n",
    "                    a.`Female Population` as female_population,\n",
    "                    a.`Total Population` as total_population,\n",
    "                    a.`Number of Veterans` as veteran_population,\n",
    "                    a.`Foreign-born` as foreign_population,\n",
    "                    a.`Average Household Size` as avg_household_size,\n",
    "                    a.`State Code` as state_code,\n",
    "                    a.Race as race\n",
    "                    FROM df a INNER JOIN (SELECT city,MAX(count) as count\n",
    "                                            FROM df\n",
    "                                            GROUP BY city) b\n",
    "                    ON a.city = b.city AND a.count = b.count\n",
    "                    \"\"\")\n",
    "df_us_cities.createOrReplaceTempView(\"df_us_cities\")\n",
    "df_us_cities.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### airport-codes_csv data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('airport-codes_csv.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning airport-codes_csv data and only extracting US airports data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Missing Values:** This data has missing values in many columns. \n",
    "- **Useless Columns:** This data has many useless columns which are either completely empty or have majority of missing values.\n",
    "- **Column Names:** Column names have been abbreviated and confusing It would be better to give them more precise names.\n",
    "- **Coded Columns** Many columns have coded values in them. One such column is iso_region.\n",
    "- **Unnecessary Rows:** One thing to notice here that this a is global data but I only require information about US airports for my dimension table associated to this data. So I am going to drop all other airports except for the ones with US data.\n",
    "- **iso_region:** One other thing to notice here that values in iso_region column are coded with coutry code(US for US) as well as state Codes and separated by a \"-\" . As I am only interested in US data so I am going to parse this column and only extratc state codes using regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+------+------------+--------------------+\n",
      "|airport_id|         type|                name|elevation_ft|region|municipality|         coordinates|\n",
      "+----------+-------------+--------------------+------------+------+------------+--------------------+\n",
      "|       00A|     heliport|   Total Rf Heliport|          11|    PA|    Bensalem|-74.9336013793945...|\n",
      "|      00AA|small_airport|Aero B Ranch Airport|        3435|    KS|       Leoti|-101.473911, 38.7...|\n",
      "|      00AK|small_airport|        Lowell Field|         450|    AK|Anchor Point|-151.695999146, 5...|\n",
      "|      00AL|small_airport|        Epps Airpark|         820|    AL|     Harvest|-86.7703018188476...|\n",
      "|      00AR|       closed|Newport Hospital ...|         237|    AR|     Newport| -91.254898, 35.6087|\n",
      "|      00AS|small_airport|      Fulton Airport|        1100|    OK|        Alex|-97.8180194, 34.9...|\n",
      "|      00AZ|small_airport|      Cordes Airport|        3810|    AZ|      Cordes|-112.165000915527...|\n",
      "|      00CA|small_airport|Goldstone /Gts/ A...|        3038|    CA|     Barstow|-116.888000488, 3...|\n",
      "|      00CL|small_airport| Williams Ag Airport|          87|    CA|       Biggs|-121.763427, 39.4...|\n",
      "|      00CN|     heliport|Kitchen Creek Hel...|        3350|    CA| Pine Valley|-116.4597417, 32....|\n",
      "|      00CO|       closed|          Cass Field|        4830|    CO|  Briggsdale|-104.344002, 40.6...|\n",
      "|      00FA|small_airport| Grass Patch Airport|          53|    FL|    Bushnell|-82.2190017700195...|\n",
      "|      00FD|     heliport|  Ringhaver Heliport|          25|    FL|   Riverview|-82.3453979492187...|\n",
      "|      00FL|small_airport|   River Oak Airport|          35|    FL|  Okeechobee|-80.9692001342773...|\n",
      "|      00GA|small_airport|    Lt World Airport|         700|    GA|    Lithonia|-84.0682983398437...|\n",
      "|      00GE|     heliport|    Caffrey Heliport|         957|    GA|       Hiram|-84.7339019775390...|\n",
      "|      00HI|     heliport|  Kaupulehu Heliport|          43|    HI| Kailua/Kona|-155.980233, 19.8...|\n",
      "|      00ID|small_airport|Delta Shores Airport|        2064|    ID|  Clark Fork|-116.213996887207...|\n",
      "|      00IG|small_airport|       Goltl Airport|        3359|    KS|    McDonald|-101.395994, 39.7...|\n",
      "|      00II|     heliport|Bailey Generation...|         600|    IN|  Chesterton|-87.122802734375,...|\n",
      "+----------+-------------+--------------------+------------+------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"airport-codes_csv.csv\",sep=\",\", inferSchema=True, header=True)\n",
    "def parseregion(line):\n",
    "    import re\n",
    "    pattern = \"\\w{2}-(\\w{2})\"\n",
    "    match = re.search(pattern,line)\n",
    "\n",
    "    return match.group(1)\n",
    "spark.udf.register('parseregion',parseregion,Str())\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "df_airports = spark.sql(\"\"\"SELECT ident as airport_id,\n",
    "                type,\n",
    "                name,\n",
    "                elevation_ft,\n",
    "                parseregion(iso_region) as region,\n",
    "                municipality,\n",
    "                coordinates\n",
    "                FROM df\n",
    "                WHERE iso_country = 'US'\n",
    "                \"\"\")\n",
    "df_airports.createOrReplaceTempView(\"df_airports\")\n",
    "df_airports.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the Data Model:\n",
    "Ok Now that my data is all cleaned and ready its time to define my Data model.\n",
    "### 3.1 Conceptual Data Model\n",
    "Since the purpose of this data warehouse is for OLAP and BI app usage, we will model these data sets with star schema data modeling.\n",
    "![ER_Diagram](ER_Diagram.png)\n",
    "\n",
    "### 3.2 Mapping Out Data Pipelines\n",
    "1. Assume all data sets are stored in S3 buckets as below (This step is for etl.py file, Although in this notebook I am assuming the same path as specified in workspace).\n",
    "    - [Source_S3_Bucket]/immigration/18-83510-I94-Data-2016/*.sas7bdat\n",
    "    - [Source_S3_Bucket]/I94_SAS_Labels_Descriptions.SAS\n",
    "    - [Source_S3_Bucket]/us-cities-demographics.csv\n",
    "    - [Source_S3_Bucket]/airport-codes_csv.csv\n",
    "2. Follow by Step 2 â€“ Cleaning step to clean up data sets or You can go directly to etl.py and run it by uploading data in s3 bucket as specified above.\n",
    "3. Once the data is cleaned you can follow the steps specified below for creating the above mentioned star schema.\n",
    "4. One Thing to note is that in pipeline on my jupyter notebook I have not written my data to a perquet file in s3 however in my etl.py file I have written my tables to perquet files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Pipelines to Model the Data \n",
    "### 4.1 Create the data model\n",
    "![ER](ER_Diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fact_immigration table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here\n",
    "fact_immigration=spark.sql(\"\"\"SELECT id,\n",
    "                                cicid,\n",
    "                                year,\n",
    "                                month,\n",
    "                                port,\n",
    "                                state_code,\n",
    "                                visa,\n",
    "                                arrival_date,\n",
    "                                departure_date,\n",
    "                                adm_num\n",
    "                                FROM df_immi\"\"\")\n",
    "fact_immigration.createOrReplaceTempView('fact_immigration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimm_immi_personal table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimm_immi_personal = spark.sql(\"\"\"\n",
    "                                SELECT DISTINCT i.cicid,\n",
    "                                        cc.country_name as citizen_country,\n",
    "                                        rc.country_name as residence_country,\n",
    "                                        i.birth_year,\n",
    "                                        i.gender\n",
    "                                        FROM df_immi i\n",
    "                                        LEFT JOIN df_countries cc ON i.citizen_country=cc.country_code\n",
    "                                        LEFT JOIN df_countries rc ON i.residence_country=rc.country_code\n",
    "                                        ORDER BY cicid\n",
    "                                        \n",
    "                                \"\"\")\n",
    "dimm_immi_personal.createOrReplaceTempView('dimm_immi_personal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimm_flight_detail table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimm_flight_detail = spark.sql(\"\"\"\n",
    "                                SELECT DISTINCT adm_num,\n",
    "                                        flight_no,\n",
    "                                        airline,\n",
    "                                        visatype\n",
    "                                FROM df_immi\"\"\")\n",
    "dimm_flight_detail.createOrReplaceTempView('dimm_flight_detail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimm_airports table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimm_airports = spark.sql(\"\"\"SELECT * FROM df_airports\"\"\")\n",
    "dimm_airports.createOrReplaceTempView('dimm_airports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimm_city_population table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimm_city_population = spark.sql(\"\"\"\n",
    "                                    SELECT uc.city,\n",
    "                                            uc.state,\n",
    "                                            c.city_code,\n",
    "                                            uc.state_code,\n",
    "                                            uc.male_population,\n",
    "                                            uc.female_population,\n",
    "                                            uc.total_population,\n",
    "                                            uc.veteran_population,\n",
    "                                            uc.foreign_population,\n",
    "                                            uc.race\n",
    "                                            FROM df_us_cities uc\n",
    "                                            LEFT JOIN df_city c ON uc.city=c.city_name\n",
    "                                            WHERE city_code IS NOT NULL\n",
    "                                            \"\"\")\n",
    "\n",
    "# dimm_city_population = dimm_city_population.dropDuplicates(['city'])\n",
    "dimm_city_population.createOrReplaceTempView('dimm_city_population')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimm_city_stats table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimm_city_stats = spark.sql(\"\"\"SELECT uc.city,\n",
    "                                uc.state,\n",
    "                                c.city_code,\n",
    "                                uc.state_code,\n",
    "                                uc.median_age,\n",
    "                                uc.avg_household_size\n",
    "                                FROM  df_us_cities uc\n",
    "                                LEFT JOIN df_city c ON uc.city=c.city_name\n",
    "                                WHERE city_code IS NOT NULL\"\"\")\n",
    "dimm_city_stats.createOrReplaceTempView('dimm_city_stats_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data Quality Checks\n",
    "Data quality checks includes:\n",
    "    \n",
    "1. Data schema of every dimensional table matches data model.\n",
    "2. No empty table after running ETL data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data schema of every dimensional table matches data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- fact_immigration_table ------------------------------------------\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- cicid: long (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- port: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- visa: string (nullable = true)\n",
      " |-- arrival_date: timestamp (nullable = true)\n",
      " |-- departure_date: timestamp (nullable = true)\n",
      " |-- adm_num: long (nullable = true)\n",
      "\n",
      "-------------------------------- dimm_imm_personal_table -----------------------------------------\n",
      "root\n",
      " |-- cicid: long (nullable = true)\n",
      " |-- citizen_country: string (nullable = true)\n",
      " |-- residence_country: string (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n",
      "-------------------------------- dimm_flight_detail_table ----------------------------------------\n",
      "root\n",
      " |-- adm_num: long (nullable = true)\n",
      " |-- flight_no: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "-------------------------------- dimm_airports_table ---------------------------------------------\n",
      "root\n",
      " |-- airport_id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "-------------------------------- dimm_city_population_table --------------------------------------\n",
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- city_code: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- male_population: integer (nullable = true)\n",
      " |-- female_population: integer (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- veteran_population: integer (nullable = true)\n",
      " |-- foreign_population: integer (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      "\n",
      "-------------------------------- dimm_city_stats_table -------------------------------------------\n",
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- city_code: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- avg_household_size: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------------- fact_immigration_table ------------------------------------------\")\n",
    "fact_immigration.printSchema()\n",
    "print(\"-------------------------------- dimm_imm_personal_table -----------------------------------------\")\n",
    "dimm_immi_personal.printSchema()\n",
    "print(\"-------------------------------- dimm_flight_detail_table ----------------------------------------\")\n",
    "dimm_flight_detail.printSchema()\n",
    "print(\"-------------------------------- dimm_airports_table ---------------------------------------------\")\n",
    "dimm_airports.printSchema()\n",
    "print(\"-------------------------------- dimm_city_population_table --------------------------------------\")\n",
    "dimm_city_population.printSchema()\n",
    "print(\"-------------------------------- dimm_city_stats_table -------------------------------------------\")\n",
    "dimm_city_stats.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No empty table after running ETL data pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- fact_immigration_table ------------------------------------------\n",
      "Table is not empty: total 2377967 records.\n",
      "-------------------------------- dimm_imm_personal_table -----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------------- fact_immigration_table ------------------------------------------\")\n",
    "record_num = fact_immigration.count()\n",
    "if record_num <= 0:\n",
    "    raise ValueError(\"This table is empty!\")\n",
    "else:\n",
    "    print(f\"Table is not empty: total {record_num} records.\")\n",
    "print(\"-------------------------------- dimm_imm_personal_table -----------------------------------------\")\n",
    "record_num = dimm_immi_personal.count()\n",
    "if record_num <= 0:\n",
    "    raise ValueError(\"This table is empty!\")\n",
    "else:\n",
    "    print(f\"Table is not empty: total {record_num} records.\")\n",
    "print(\"-------------------------------- dimm_flight_detail_table ----------------------------------------\")\n",
    "recod_num = dimm_flight_detail.count()\n",
    "if record_num <= 0:\n",
    "    raise ValueError(\"This table is empty!\")\n",
    "else:\n",
    "    print(f\"Table is not empty: total {record_num} records.\")\n",
    "print(\"-------------------------------- dimm_airports_table ---------------------------------------------\")\n",
    "record_num = dimm_airports.count()\n",
    "if record_num <= 0:\n",
    "    raise ValueError(\"This table is empty!\")\n",
    "else:\n",
    "    print(f\"Table is not empty: total {record_num} records.\")\n",
    "print(\"-------------------------------- dimm_city_population_table --------------------------------------\")\n",
    "record_num = dimm_city_population.count()\n",
    "if record_num <= 0:\n",
    "    raise ValueError(\"This table is empty!\")\n",
    "else:\n",
    "    print(f\"Table is not empty: total {record_num} records.\")\n",
    "print(\"-------------------------------- dimm_city_stats_table -------------------------------------------\")\n",
    "record_num = dimm_city_stats.count()\n",
    "if record_num <= 0:\n",
    "    raise ValueError(\"This table is empty!\")\n",
    "else:\n",
    "    print(f\"Table is not empty: total {record_num} records.\")\n",
    "print(\"-------------------------------- Test Complete ---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Data dictionary:\n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fact_immigration_table:\n",
    "immigrant log fact table:\n",
    "- id ------------------------------ Autogenerated Unique id for fact table.\n",
    "- cicid --------------------------- Unique id for fact table provided in original data.\n",
    "- year ---------------------------- 4 digit year.\n",
    "- month --------------------------- numeric month.\n",
    "- port ---------------------------- city_code provided in i94 immigration data, city name can be retrieved                                     from dimm_city_population table.\n",
    "- state_code ---------------------- state_code for states in US, State name can be retrieved from                                               dimm_city_population table.\n",
    "- visa ---------------------------- visa cetagory.\n",
    "- arrival_date -------------------- arrival timestamp for immigrant.\n",
    "- departure_date ------------------ departure date for immigrant.\n",
    "- adm_num ------------------------- admission number of immigrant.\n",
    "\n",
    "#### dimm_immi_personel\n",
    "personal detail for each immigrant:\n",
    "- cicid ---------------------------- Unique id for fact table provided in original data.\n",
    "- citizen_country ------------------ citizenship country of immigrant.\n",
    "- residence_country ---------------- residence country of immigrant.\n",
    "- birth_year ----------------------- birth year of immigrant.\n",
    "- gender --------------------------- gender of immigrant.\n",
    "\n",
    "#### dimm_flight_detail\n",
    "flight detail for given admission number:\n",
    "- adm_num ------------------------- admission number of immigrant.\n",
    "- flight_no ----------------------- Number of flight.\n",
    "- airline ------------------------- Airline flight belongs to.\n",
    "- visatype ------------------------ Visatype of the pessenger.\n",
    "\n",
    "#### dimm_city_stats\n",
    "statistic for US cities:\n",
    "- city_code ------------------------ City code for each city.\n",
    "- city ----------------------------- Name of each city.\n",
    "- state ---------------------------- state name of each city.\n",
    "- state_code ----------------------- state code of each city.\n",
    "- median_age ----------------------- median age for each city.\n",
    "- avg_household_size --------------- Average household size of each city.\n",
    "\n",
    "#### dimm_city_population:\n",
    "population numbers for US cities:\n",
    "- city_code ------------------------ City code for each city.\n",
    "- city ----------------------------- Name of each city.\n",
    "- state ---------------------------- state name of each city.\n",
    "- state_code ----------------------- state code of each city.\n",
    "- male_population ------------------ Male population of city.\n",
    "- female_population ---------------- Female population of city.\n",
    "- total_population ----------------- Total population of city.\n",
    "- veteran_population --------------- Veteran population of city.\n",
    "- foreign_population --------------- foreign citizens population of city.\n",
    "- race ----------------------------- Majority ethnicity of city.\n",
    "\n",
    "#### dimm_airports:\n",
    "airports in United states:\n",
    "- airport_id ----------------------- Airport id of airport.\n",
    "- type ----------------------------- type of airport.\n",
    "- name ----------------------------- name of airport.\n",
    "- elevation_ft --------------------- alevation of airport in feet.\n",
    "- region --------------------------- US State in which Airport is present.\n",
    "- municipality --------------------- Municipality of Airport.\n",
    "- coordinates ---------------------- Coordinates of Airport."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Complete Project Write Up\n",
    "### Tools and technologies:\n",
    "- AWS S3 for data storage-------------- I chose AWS S3 for data storage because it is easier maintain.\n",
    "- Pandas ------------------------------ I used Pandas because pandas dataframes are easier to visualize and explore.\n",
    "- Pyspark ----------------------------- I used Pyspark because of its schema on read features.\n",
    "\n",
    "### Steps Taken:\n",
    "- I imported pyspark and pandas libraries which are necessary for this project.\n",
    "- I decided the scope of the project. i.e. I decided to use i94 immigration data, airport codes data and us cities damography data for my project.\n",
    "- I created a spark session to work with my data.\n",
    "- I loaded the datasets for my project.\n",
    "- I cleaned those datasets by dropping irrelevant datam null values and duplicate values using schema on read features in pyspark.\n",
    "- I also used advanced annalytics nlp methods on i9_SAS_labels_description to extract countries, cities and state names for their provided codes in the datasets.\n",
    "- I replaced these codes with their relevant names where necessary in the original datasets.\n",
    "- I modeled a star schema for my ETL pipeline.\n",
    "- I created tables as specfied in star schema from the original datasets.\n",
    "- i finalized my etl pipeline by doing data quality checks of my data warehouse.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The Reason I finalized this data model:\n",
    "I finalized this data model because I wanted to generate some beneficial OLAP cubes which would be easier to communicate with relevant authorities .e.g. some official from department of home affairs want to know how many female immigrants ported the city with highest foreign population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------------+------------------+\n",
      "|    city|porting_female_immigrants|foreign_population|\n",
      "+--------+-------------------------+------------------+\n",
      "|NEW YORK|                   192665|           3212500|\n",
      "+--------+-------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT c.city, count(f.id) as porting_female_immigrants,c.foreign_population\n",
    "            FROM fact_immigration f\n",
    "            LEFT JOIN dimm_city_population c\n",
    "            ON f.port = c.city_code\n",
    "            LEFT JOIN dimm_immi_personal p\n",
    "            ON f.cicid = p.cicid\n",
    "            WHERE p.gender = 'F'\n",
    "            GROUP BY c.city,c.foreign_population\n",
    "            ORDER BY c.foreign_population DESC\n",
    "            LIMIT 1\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Update Frequency:\n",
    "- Tables created from immigration data set should be updated monthly since the raw data set is built up monthly.\n",
    "- Tables created from demography data set could be updated annually since demography data collection takes time and high frequent demography might take high cost but generate wrong conclusion.\n",
    "- Table generated from airport codes data should be update annually as well, since airports are not created  or shut down overnight and frequency of new airports adding to a location is quite low.\n",
    "\n",
    "### How to approach the problem differently under the following scenarios:\n",
    "- **The data was increased by 100x.**\n",
    "    If data is increased by 100x then we should consider to put data in AWS EMR which is a distributed data cluster for processing large data sets on cloud.\n",
    " \n",
    "- **The data populates a dashboard that must be updated on a daily basis by 7am every day.**\n",
    "    Apache Airflow is a usefull too for building up a ETL data pipeline to update the date regularly. Apache Airflow integrated with Python and AWS very well. Many applications can be combined together to deliver task automation.\n",
    "- **The database needed to be accessed by 100+ people.**\n",
    "    AWS Redshift is a good consideration in this case as Redshift can easily handle upto 500 connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
